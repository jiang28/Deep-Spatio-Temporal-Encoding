{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiang28/Deep-Spatio-Temporal-Encoding/blob/master/TransGcnRE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsodPZHaJ9qc",
        "outputId": "c29a5761-c2e5-40b0-ef36-5c753f120b6e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AoW1WhKOl1f",
        "outputId": "e3380034-35d2-418a-d54b-7555e05229ab"
      },
      "source": [
        "cd drive/MyDrive/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpjabYjuOsyT",
        "outputId": "067730aa-049b-41be-c14d-10ff27c08535"
      },
      "source": [
        "PATH = '/content/drive/My Drive/'\n",
        "device = 'cuda'\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "  device = 'cuda'\n",
        "else:\n",
        "  device = 'cpu'\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5dPztY2Y7Oa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b5bbbc4-1f67-4f43-d1a1-7c97aaf02dff"
      },
      "source": [
        "#load data\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from pandas import read_csv\n",
        "\n",
        "output_window = 1\n",
        "\n",
        "def get_data():\n",
        "    data = []\n",
        "    features = []\n",
        "    location = []\n",
        "    label = []\n",
        "    series = read_csv('DATA/data_census_update_school.csv', sep=',', header=None, low_memory=False, keep_default_na=False).to_numpy()\n",
        "\n",
        "    for i in range(len(series)):\n",
        "        if i == 0:\n",
        "            continue\n",
        "\n",
        "        # if '' in series[i]:\n",
        "        #   continue\n",
        "\n",
        "        line = series[i][2:-11]\n",
        "        line = [float(_) if _ != '' else -1 for _ in line]\n",
        "        line = [int(j) for j in line]\n",
        "\n",
        "        data.append(line)\n",
        "\n",
        "        f = series[i][-11:-1]\n",
        "        f = [float(_) if _ != '' else -1 for _ in f]\n",
        "        f = [int(_) for _ in f]\n",
        "\n",
        "        features.append(f)\n",
        "\n",
        "        label.append(int(float(series[i][-1])))\n",
        "\n",
        "    data = np.asarray(data)\n",
        "    features = np.asarray(features)\n",
        "    label = np.asarray(label)\n",
        "\n",
        "    data_ = []\n",
        "    features_ = []\n",
        "    for i in range(len(data)):\n",
        "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "        x = data[i]\n",
        "        x = scaler.fit_transform(np.array(x).reshape(-1, 1)).reshape(-1)\n",
        "        data_.append(x)\n",
        "\n",
        "        features_.append(scaler.fit_transform(np.array(features[i]).reshape(-1, 1)).reshape(-1))\n",
        "\n",
        "    input_data = torch.FloatTensor(data_)\n",
        "    label = torch.FloatTensor(label)\n",
        "    features = torch.FloatTensor(features_)\n",
        "    print(len(input_data))\n",
        "    samples = int(len(input_data)*0.8)\n",
        "    x_train = input_data[:samples]\n",
        "    x_test = input_data[samples:]\n",
        "\n",
        "    y_train = label[:samples]\n",
        "    y_test = label[samples:]\n",
        "\n",
        "    ftr_train = features[:samples]\n",
        "    ftr_test = features[samples:]\n",
        "\n",
        "    return x_train, y_train, ftr_train, x_test, y_test, ftr_test, features\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    x_train, y_train, ftr_train, x_test, y_test, ftr_test, ftr_all = get_data()\n",
        "    #print(x_train.shape)\n",
        "    #print(y_train.shape)\n",
        "    #print(ftr_train.shape)\n",
        "\n",
        "    #print(x_test.shape)\n",
        "    #print(y_test.shape)\n",
        "    #print(ftr_test.shape)\n",
        "\n",
        "    #print(ftr_all.shape)\n",
        "\n",
        "    pass\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7436\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:50: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMiyIWx-Ouvq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c7b40fd-9b96-4f85-fd91-b777623c05dd"
      },
      "source": [
        "#load data\n",
        "import torch\n",
        "def load_matrix():\n",
        "    matrix = []\n",
        "    with open('DATA/matrix.txt', 'r') as f:\n",
        "        for i, num in enumerate(f):\n",
        "            if i % 7436 == 0:\n",
        "                if i != 0:\n",
        "                    matrix.append(row)\n",
        "                row = []\n",
        "            row.append(int(num))\n",
        "    matrix.append(row)\n",
        "    adj = torch.FloatTensor(matrix)\n",
        "    print('adj_matrix', adj[0])\n",
        "\n",
        "    # samples = int(7436 * 0.7)\n",
        "    # loc_train = range(samples)\n",
        "    # loc_test = range(samples, 7436)\n",
        "    # loc_val = loc_test\n",
        "\n",
        "    # loc_train = torch.Tensor(loc_train)\n",
        "    # loc_val = torch.Tensor(loc_val)\n",
        "    # loc_test = torch.Tensor(loc_test)\n",
        "\n",
        "    # loc_train = loc_train.unsqueeze(0)\n",
        "    # loc_val = loc_val.unsqueeze(0)\n",
        "    # loc_test = loc_test.unsqueeze(0)\n",
        "\n",
        "    return adj\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    adj = load_matrix()\n",
        "    print(adj.shape)\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "adj_matrix tensor([-1.,  4.,  4.,  ...,  0.,  0.,  0.])\n",
            "torch.Size([7436, 7436])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuXbZ6SFPrkV"
      },
      "source": [
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import argparse\n",
        "import sys\n",
        "import math\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules.module import Module\n",
        "\n",
        "class GCNLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super(GCNLayer, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        support = torch.mm(input, self.weight)\n",
        "        output = torch.spmm(adj, support)\n",
        "        if self.bias is not None:\n",
        "            return output + self.bias\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' \\\n",
        "               + str(self.in_features) + ' -> ' \\\n",
        "               + str(self.out_features) + ')'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UD-bSFIWa8d"
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        pe.requires_grad = True\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(0), :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbpuKtpLck5r"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class TransGCN(nn.Module):\n",
        "    def __init__(self, input_size, feature_size=250,num_layers=1,dropout=0.1):\n",
        "        super(TransGCN, self).__init__()\n",
        "\n",
        "        self.gc1 = GCNLayer(10, 10)\n",
        "        # self.gc2 = GCNLayer(5, 250)\n",
        "\n",
        "\n",
        "        self.src_mask = None\n",
        "        self.pos_encoder = PositionalEncoding(feature_size)\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size, nhead=10, dropout=dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
        "\n",
        "\n",
        "        #for layer in self.transformer_encoder.children():\n",
        "            #if isinstance(layer, nn.Linear):\n",
        "                #print(layer.state_dict()['weight'])\n",
        "\n",
        "#####################################################################################################\n",
        "\n",
        "\n",
        "        #num_layers = self.transformer_encoder.num_layers\n",
        "        #d_model = self.transformer_encoder.layers[0].self_attn.embed_dim\n",
        "        #num_heads = self.transformer_encoder.layers[0].self_attn.num_heads\n",
        "        #norm_first = self.transformer_encoder.layers[0].norm_first\n",
        "\n",
        "        #batch_size = 32\n",
        "        #seq_len = 7436\n",
        "\n",
        "\n",
        "        #x = torch.randn((batch_size,seq_len,d_model))\n",
        "\n",
        "        #attention_maps = []\n",
        "\n",
        "        #with torch.no_grad():\n",
        "            #for i in range(num_layers):\n",
        "                # compute attention of layer i\n",
        "                #h = x.clone()\n",
        "                #if norm_first:\n",
        "                    #h = self.transformer_encoder.layers[i].norm1(h)\n",
        "                    #attn = self.transformer_encoder.layers[i].self_attn(h, h, h,attn_mask=self.src_mask,need_weights=True)[1]\n",
        "                    #attention_maps.append(attn)\n",
        "                    # forward of layer i\n",
        "                    #x = self.transformer_encoder.layers[i](x,src_mask=self.src_mask)\n",
        "        #print(attention_maps)\n",
        "        ######################################################################################\n",
        "\n",
        "        self.decoder = nn.Linear(feature_size,1)\n",
        "        self.layer1 = nn.Linear(input_size, 200)\n",
        "        self.layer2 = nn.Linear(200, 100)\n",
        "        self.layer3 = nn.Linear(100, 1)\n",
        "\n",
        "        self.last_layer = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, src, ftr_all, adj, st_idx, ed_idx):\n",
        "        #gcn embedding loc information\n",
        "        #print('gcn input shape', ftr_all.shape)\n",
        "        gcn_output = self.gc1(ftr_all, adj)\n",
        "        #print('gcn output shape', gcn_output.shape)\n",
        "        gcn_output = F.relu(gcn_output)\n",
        "        # gcn_tmp = F.relu(self.gc1(ftr_all, adj))\n",
        "        # gcn_output = self.gc2(gcn_tmp, adj)\n",
        "\n",
        "        #transformer embedding time series data\n",
        "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
        "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
        "            self.src_mask = mask\n",
        "\n",
        "        #print('before pos', src.shape)\n",
        "        src = self.pos_encoder(src)\n",
        "\n",
        "        #print('after pos', src.shape)\n",
        "\n",
        "        hidden = self.transformer_encoder(src,self.src_mask)\n",
        "\n",
        "        #print('after encoder', src.shape)\n",
        "\n",
        "        gcn_output = gcn_output[st:ed]\n",
        "        gcn_output = torch.transpose(gcn_output, 0, 1)\n",
        "\n",
        "        gcn_output = torch.reshape(gcn_output, (gcn_output.shape[0], gcn_output.shape[1], 1))\n",
        "        gcn_output = gcn_output.expand(gcn_output.shape[0], gcn_output.shape[1], hidden.shape[2])\n",
        "\n",
        "        # concate gcn output to transformer output\n",
        "        output = torch.cat((hidden, gcn_output), 0)\n",
        "\n",
        "        output = self.decoder(output)\n",
        "\n",
        "        output = torch.squeeze(output)\n",
        "        output = output.transpose(0, 1)\n",
        "\n",
        "        #MLP prediction\n",
        "        output = self.layer1(output)\n",
        "        output = self.layer2(output)\n",
        "        output = self.layer3(output)\n",
        "\n",
        "        output = self.last_layer(output)\n",
        "\n",
        "        #output attention matrix\n",
        "        #print(self.transformer_encoder.layers)\n",
        "        #save_output = SaveOutput()\n",
        "        #attention_weights = self.transformer_encoder.layers[0].linear2.weight\n",
        "\n",
        "        #in_proj_weight\n",
        "        in_proj_attention_weights = self.transformer_encoder.layers[0].self_attn.in_proj_weight\n",
        "\n",
        "        out_proj_attention_weights = self.transformer_encoder.layers[0].self_attn.out_proj.weight\n",
        "        #print(\"out attn map shape: \", out_proj_attention_weights.shape)\n",
        "\n",
        "        q_proj_attention_weights = self.transformer_encoder.layers[0].self_attn.q_proj_weight\n",
        "\n",
        "        k_proj_attention_weights = self.transformer_encoder.layers[0].self_attn.k_proj_weight\n",
        "\n",
        "        v_proj_attention_weights = self.transformer_encoder.layers[0].self_attn.v_proj_weight\n",
        "\n",
        "\n",
        "        #attention_weights = self.transformer_encoder.weight\n",
        "        in_proj_attn_array = in_proj_attention_weights.cpu().detach().numpy()\n",
        "        out_proj_attn_array = out_proj_attention_weights.cpu().detach().numpy()\n",
        "        #print(\"out attn map shape: \", out_proj_attn_array.shape)\n",
        "        #q_proj_attn_array = q_proj_attention_weights.cpu().detach().numpy()\n",
        "        #k_proj_attn_array = k_proj_attention_weights.cpu().detach().numpy()\n",
        "        #v_proj_attn_array = v_proj_attention_weights.cpu().detach().numpy()\n",
        "        #print(q_proj_attention_weights)\n",
        "\n",
        "        #save numpy to csv\n",
        "\n",
        "        import numpy\n",
        "        import pandas as pd\n",
        "        #numpy.savetxt(\"in_proj_attn_array\", in_proj_attn_array, delimiter=\",\")\n",
        "        #numpy.savetxt(\"out_proj_attn_array\", out_proj_attn_array, delimiter=\",\")\n",
        "        #numpy.savetxt(\"q_proj_attn_array\", q_proj_attn_array, delimiter=\",\")\n",
        "        #numpy.savetxt(\"k_proj_attn_array\", k_proj_attn_array, delimiter=\",\")\n",
        "        #numpy.savetxt(\"v_proj_attn_array\", v_proj_attn_array, delimiter=\",\")\n",
        "\n",
        "        list_of_numbers = list(range(0, 250))\n",
        "\n",
        "        df3 = pd.DataFrame(out_proj_attn_array, columns=list_of_numbers)\n",
        "        #df3.to_csv('DATA/out.csv')\n",
        "        #print('df.shape: ', df.shape)\n",
        "\n",
        "        import numpy as np\n",
        "        import seaborn as sns\n",
        "        import matplotlib.pylab as plt\n",
        "\n",
        "        #plot heatmap\n",
        "        #g=sns.heatmap(df3, xticklabels = list_of_numbers)\n",
        "        #plt.show()\n",
        "        #save_output = SaveOutput()\n",
        "        #attention_weights = self.transformer_encoder.layers[-1].self_attn.register_forward_hook(save_output)\n",
        "        #print(attn_array)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "if __name__ == '__main__':\n",
        "    arg_parser = argparse.ArgumentParser()\n",
        "    arg_parser.add_argument('--mode', choices=['train', 'infer'],\\\n",
        "        default='train',help='Run mode')\n",
        "    arg_parser.add_argument('--epoch', default='200', type=int)\n",
        "    arg_parser.add_argument('--batch_size', default='32', type=int)\n",
        "    args = arg_parser.parse_args(args=['--mode', 'train'])\n",
        "    args = arg_parser.parse_args(args=['--epoch', '200'])\n",
        "    args = arg_parser.parse_args(args=['--batch_size', '32'])\n",
        "\n",
        "    X_train, y_train, FTR_train, X_test, y_test, FTR_test, FTR_all = get_data()\n",
        "\n",
        "    X_train = X_train.to(device)\n",
        "    y_train = y_train.to(device)\n",
        "    FTR_train = FTR_train.to(device)\n",
        "    X_test = X_test.to(device)\n",
        "    y_test = y_test.to(device)\n",
        "    FTR_test = FTR_test.to(device)\n",
        "\n",
        "    ftr_all = FTR_all.to(device)\n",
        "\n",
        "    #print(X_train.shape)\n",
        "    #print(y_train.shape)\n",
        "    #print(X_test.shape)\n",
        "    #print(y_test.shape)\n",
        "    #print(ftr_all.shape)\n",
        "\n",
        "    adj = load_matrix()\n",
        "\n",
        "    adj = adj.to(device)\n",
        "    # loc_train = loc_train.to(device)\n",
        "    # loc_val = loc_val.to(device)\n",
        "    # loc_test = loc_test.to(device)\n",
        "\n",
        "    # print(adj.shape)\n",
        "    # print(loc_train.shape)\n",
        "    # print(loc_val.shape)\n",
        "    # print(loc_test.shape)\n",
        "\n",
        "    model_path = PATH + 'MODEL/model_trans_gcn.pt'\n",
        "    model = TransGCN(input_size = X_train.shape[1] + ftr_all.shape[1])\n",
        "    model.to(device)\n",
        "\n",
        "\n",
        "    loss_function = nn.MSELoss()\n",
        "\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.02)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.98)\n",
        "\n",
        "    num_batch = int(len(y_train)/args.batch_size) + 1\n",
        "\n",
        "    num_batch_test = int(len(y_test)/args.batch_size) + 1\n",
        "\n",
        "    last_acc = 0.0\n",
        "\n",
        "    if args.mode == 'train':\n",
        "\n",
        "        for epoch in range(args.epoch):\n",
        "\n",
        "            model.to(device)\n",
        "\n",
        "            acc = []\n",
        "            total_loss = 0\n",
        "\n",
        "            acc_test = []\n",
        "\n",
        "            for i in range(num_batch):\n",
        "                sys.stdout.write('\\r{0}/{1}'.format(i, num_batch))\n",
        "                st = i * args.batch_size\n",
        "                ed = min((i+1) * args.batch_size, len(y_train))\n",
        "\n",
        "                x = X_train[st:ed]\n",
        "                x = x.transpose(0, 1)\n",
        "                x = torch.reshape(x, (x.shape[0], x.shape[1], 1))\n",
        "                x = x.to(device)\n",
        "\n",
        "                # ftr = FTR_train[st:ed]\n",
        "                # ftr = ftr.transpose(0, 1)\n",
        "                # ftr = torch.reshape(ftr, (ftr.shape[0], ftr.shape[1], 1))\n",
        "                # ftr = ftr.to(device)\n",
        "\n",
        "                label = y_train[st:ed]\n",
        "                label = label.to(device)\n",
        "\n",
        "                model.zero_grad()\n",
        "\n",
        "                # junwei\n",
        "                #print('\\n input size', x.shape)\n",
        "                output = model(x, ftr_all, adj, st, ed)\n",
        "\n",
        "                predict = torch.squeeze(output)\n",
        "\n",
        "                loss = loss_function(predict, label)\n",
        "\n",
        "                acc.append(torch.sum(predict.gt(0.5) == label))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            for i in range(num_batch_test):\n",
        "                st = i * args.batch_size\n",
        "                ed = min((i+1) * args.batch_size, len(y_test))\n",
        "\n",
        "                x_test = X_test[st:ed]\n",
        "                x_test = x_test.transpose(0, 1)\n",
        "                x_test = torch.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
        "                x_test = x_test.to(device)\n",
        "\n",
        "                # ftr_test = FTR_test[st:ed]\n",
        "                # ftr_test = ftr_test.transpose(0, 1)\n",
        "                # ftr_test = torch.reshape(ftr_test, (ftr_test.shape[0], ftr_test.shape[1], 1))\n",
        "                # ftr_test = ftr_test.to(device)\n",
        "\n",
        "                label_test = y_test[st:ed]\n",
        "                label_test = label_test.to(device)\n",
        "\n",
        "                # output_test = model(x_test, ftr_test, loc_test, adj)\n",
        "                output_test = model(x_test, ftr_all, adj, X_train.shape[0] + st, X_train.shape[0] + ed)\n",
        "                predict_test = torch.squeeze(output_test)\n",
        "                acc_test.append(torch.sum(predict_test.gt(0.5) == label_test))\n",
        "\n",
        "            total_loss /= len(y_train)\n",
        "            acc = sum(acc)*1.0/len(y_train)\n",
        "\n",
        "            acc_test = sum(acc_test)*1.0/len(y_test)\n",
        "            if epoch%10 == 0 or epoch == args.epoch - 1:\n",
        "              print('\\nEpoch: ', epoch)\n",
        "              print('\\nTraining set: Loss {0:.4f}. Acc {1:.4f}.\\nTest set: Acc {2:.4f}.'.format(total_loss, acc, acc_test))\n",
        "\n",
        "            scheduler.step()\n",
        "\n",
        "            if acc_test > last_acc:\n",
        "                torch.save(model.cpu(), model_path)\n",
        "                last_acc = acc_test\n"
      ],
      "metadata": {
        "id": "0TKxQroAf7pL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfmLshoyMhZj"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    X_train, y_train, FTR_train, X_test, y_test, FTR_test, FTR_all = get_data()\n",
        "\n",
        "    num_batch = int(len(y_train)/args.batch_size) + 1\n",
        "\n",
        "    num_batch_test = int(len(y_test)/args.batch_size) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8S537brJ5sl"
      },
      "source": [
        "#Transformer evaluation\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "model_path = PATH + 'MODEL/model_trans_gcn.pt'\n",
        "model = torch.load(model_path)\n",
        "model.to(device)\n",
        "acc_test = []\n",
        "y_pred = []\n",
        "\n",
        "for i in range(num_batch_test):\n",
        "    st = i * args.batch_size\n",
        "    ed = min((i+1) * args.batch_size, len(y_test))\n",
        "\n",
        "    x_test = X_test[st:ed]\n",
        "    x_test = x_test.transpose(0, 1)\n",
        "    x_test = torch.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
        "    x_test = x_test.to(device)\n",
        "\n",
        "    label_test = y_test[st:ed]\n",
        "    label_test = label_test.to(device)\n",
        "\n",
        "    output_test = model(x_test, ftr_all, adj, X_train.shape[0] + st, X_train.shape[0] + ed)\n",
        "    predict_test = torch.squeeze(output_test)\n",
        "    y_pred += predict_test.gt(0.5)\n",
        "\n",
        "y_pred = list(map(float, y_pred))\n",
        "y_pred = np.asarray(y_pred)\n",
        "y_pred = torch.FloatTensor(y_pred)\n",
        "\n",
        "print('Transformer Results:')\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "print('Roc:', metrics.roc_auc_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}